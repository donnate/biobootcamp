---
title: "Lecture 6: Data modeling and linear regression"
subtitle: 'CME/STATS 195'
author: "Lan Huong Nguyen"
date: "October 16, 2018"
output: 
  revealjs::revealjs_presentation:
    self_contained: false
    lib_dir: libs
    theme: simple
    hightlights: haddock
    smart: true
    center: true
    transition: slide
    css: cme195.css
    fig_width: 8
    fig_height: 5
    reveal_options:
      slideNumber: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
library(tidyverse)
theme_set(theme_bw())
theme_update(text = element_text(size = 20))
```

## Contents


<div class="left", style="width: 50%">

* Data Modeling 

* Linear Regression

* Lasso Regression


</div>

<div class="right", style="width: 50%">
![](./Lecture6-figure/data-science-model.png)
</div>


# Data Modeling

## Introduction to models

> "**All models are wrong, but some are useful**. Now it would be very remarkable 
if any system existing in the real world could be exactly represented by any 
simple model. However, **cunningly chosen parsimonious models often do provide 
remarkably useful approximations** (...). For such a model there is no need 
to ask the question "Is the model true?". If "truth" is to be the "whole truth"
the answer must be "No". The only question of interest is "Is the model 
illuminating and useful?" -- George E.P. Box, 1976

* The goal of a model is to **provide a simple low-dimensional summary 
of a dataset**.

* Models can be used to **partition data into patterns of interest 
and residuals** (other sources of variation and random noise).


## Hypothesis generation vs. hypothesis confirmation

</br>

* Usually models are used for inference or confirmation of a pre-specified
hypothesis.

* Doing inference correctly is hard. The key idea you must understand is that:
**Each observation can either be used for exploration or confirmation, 
NOT both.**

* Observation can be used many times for exploration, but only once for 
confirmation. 

* There is nothing wrong with exploration, but you should **never sell 
an exploratory analysis as a confirmatory analysis** because it is 
fundamentally misleading.


## Confirmatory analysis


If you plan to do confirmatory analysis at some point after EDA,
one approach is to split your data into three pieces before you 
begin the analysis:

* **Training set** -- the bulk (e.g. 60%) of the dataset which can be used to
do anything: visualizing, fitting multiple models.

* **Validation set** -- a smaller set (e.g. 20%) used for manually comparing 
models and visualizations.

* **Test set** -- a set (e.g. 20%) held back used only ONCE to test and
asses your final model.


## Confirmatory analysis

- Partitioning the dataset allows you to explore the training data, generate
a number of candidate hypotheses and models.

- You can select a final model based on its performance on the validation
set.

- Finally, when you are confident with the chosen model you can
check how good it is using the test data.

- *Note that even when doing confirmatory modeling, you will still need to 
do EDA. If you don’t do any EDA you might remain blind to some quality 
problems with your data.*


## Model Basics

</br>

There are two parts to data modeling:

* **defining a family of models**: deciding on a set of models
that can express a type of pattern you want to capture, e.g. a straight line, 
or a quadratic curve.
* **fitting a model**: finding a model within the family that the closest to 
your data.

</br>

A fitted model is just the best model from a chosen family of models,
i.e. the "best" according to some set criteria.

This does not necessarily imply that the model is a good and certainly does 
NOT imply that the model is true.


## The `modelr` package

</br>

* The `modelr` package, provides a few useful functions that are wrappers 
around base R’s modeling functions.

* These functions facilitate the data analysis process
as they are nicely integrated with the `tidyverse` pipeline.

* `modelr` is not automatically loaded when you load in `tidyverse` package,
you need to do it separately:

</br>

```{r}
library(modelr)
```



## A toy dataset

We will work with a simulated dataset `sim1` from `modelr`:

<div class="left", style="width: 50%">
```{r}
sim1
```
</div>

<div class="right", style="width: 50%">

```{r}
ggplot(sim1, aes(x, y)) + geom_point()
```

</div>

## Defining a family of models

The relationship between $x$ and $y$ for the points in `sim1` look linear.
So, will look for models which belong to **a family of models** of 
the following form:


<div class="left", style="width: 50%">
</br>

$$y= \beta_0 + \beta_1 \cdot x$$

</br>

The models that can be expressed by the above formula, can adequately
capture a linear trend. 

We generate a few examples of the models from this family on the right. 

</div>

<div class="right", style="width: 50%">
```{r}
models <- tibble(
    b0 = runif(250, -20, 40),
    b1 = runif(250, -5, 5))

ggplot(sim1, aes(x, y)) + 
  geom_abline(
      data = models, 
      aes(intercept = b0, slope = b1), 
      alpha = 1/4) +
  geom_point() 
```


</div>


## Fitting a model

From all the lines in the linear family of models, we need to find the best 
one, i.e. the one that is **the closest to the data**. 

This means that we need to find parameters $\hat a_0$ and $\hat a_1$ that 
identify such a fitted line.

<div class="left", style="width: 50%">

The closest to the data can be defined as the one with the minimum distance to
the data points in the $y$ direction (the minimum residuals):

\begin{align*}
\|\hat e\|^2_2 &= \|\vec y - \hat y\|_2^2\\
&= \|\vec y - (\hat \beta_0 + \hat \beta_1 x)\|_2^2\\
&= \sum_{i = 1}^n (y_i - (\hat \beta_0 + \hat \beta_1 x_i))^2
\end{align*}


</div>

<div class="right", style="width: 50%">

```{r, echo = FALSE}
fit.lm <- lm(y ~ x, data = sim1)
sim1 <- sim1 %>% add_predictions(fit.lm)
ggplot(sim1, aes(x, y)) +
    geom_line(aes(y = pred), colour = "black", size = 1) +
    geom_segment(aes(xend = x, yend = pred), color = "royalblue", size =  1) +
    geom_point(color = "grey60") 
```


# Linear Regression

## Linear Regression

</br>

> - Regression is a supervised learning method, whose goal is inferring the 
relationship between input data, $x$, and a **continuous** response 
variable, $y$.

> - Linear regression is a type of regression where **$y$ is modeled as a 
linear function of $x$**. 

> - **Simple linear regression** predicts the output $y$ from a single predictor 
$x$. 
\[y = \beta_0 + \beta_1 x + \epsilon\]

> - **Multiple linear regression** assumes $y$ relies on many covariates:
\begin{align*}
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon \\
&= \boldsymbol{\beta}^T \boldsymbol{x} + \epsilon
\end{align*}

> - here $\epsilon$ denotes a random noise term with zero mean.

## Objective function

Linear regression seeks a solution $\hat y = \hat \beta \cdot \vec x$ 
that **minimizes the difference between the true outcome $y$ and the 
prediction $\hat y$**, in terms of the residual sum of squares (RSS).

$$
arg \min\limits_{\hat \beta} 
\sum_i \left(y_i - \boldsymbol{\hat \beta}^T \boldsymbol{x}_i\right)^2
$$


## Simple Linear Regression

* Predict the mileage per gallon using the weight of the car.

* In R the linear models can be fit with a `lm()` function.

<div class="left", style="width: 50%">

```{r}
# convert 'data.frame' to 'tibble':
mtcars <- tbl_df(mtcars)

# Separate the data into train and test:
set.seed(123)
n <- nrow(mtcars)
idx <- sample(1:n, size = floor(n/2))
mtcars_train <- mtcars[idx, ]
mtcars_test <- mtcars[-idx, ]

# Fit a simple linear model:
mtcars_fit <- lm(mpg ~ wt, mtcars_train)
# Extract the fitted model coefficients:
coef(mtcars_fit)
```
</div>

<div class="right", style="width: 50%">
```{r}
# check the details on the fitted model:
summary(mtcars_fit)
```

</div>

## Fitted values

We can compute the fitted values $\hat y$, a.k.a. the predicted `mpg` values 
for existing observations using `modelr::add_predictions()` function.

```{r}
mtcars_train <- mtcars_train %>% add_predictions(mtcars_fit)
mtcars_train
```

## Predictions for new observations

To predict the `mpg` for **new observations**, e.g. cars not in the dataset,
we first need to generate a data table with predictors $x$, in this case
the car weights:

```{r}
newcars <- tibble(wt = c(2, 2.1, 3.14, 4.1, 4.3))
newcars <- newcars %>% add_predictions(mtcars_fit)
newcars
```

## Predictions for the test set

Remember that we already set aside a test set check our model:

```{r}
mtcars_test <- mtcars_test %>% add_predictions(mtcars_fit)
head(mtcars_test, 3)
```

Compute the root mean square error:

$$
RMSE = \frac{1}{\sqrt{n}}\|\vec{y} - \vec{\hat y}\| = 
\sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - \hat y_i)^2}
$$

```{r}
sqrt(mean((mtcars_test$mpg - mtcars_test$pred)^2))
```



## Visualizing the model

Now we can compare our predictions (grey) to the observed (black) values.

```{r}
ggplot(mtcars_train, aes(wt)) + geom_point(aes(y = mpg)) +
    geom_line(aes(y = pred), color = "red", size = 1) +
    geom_point(aes(y = pred), fill = "grey", color = "black", shape = 21, size = 2) 
```


  
  
## Visualizing the residuals

**The residuals tell you what the model has missed**. We can compute and add
residuals to data with `add_residuals()` from `modelr` package:


Plotting residuals is a good practice -- you want the residuals to look 
like random noise.

<div class="left", style="width: 40%">

```{r}
mtcars_train <- mtcars_train %>% 
    add_residuals(mtcars_fit)
mtcars_train %>% 
    select(mpg, mpg, resid, pred)
```


</div>

<div class="right", style="width: 60%">


```{r}
ggplot(mtcars_train, aes(wt, resid)) + 
    geom_ref_line(h = 0, colour = "grey") +
    geom_point() 
```

</div>


## Formulae in R

You have seen that `lm()` takes in a formula relation `y ~ x` as an argument.

You can take a look at what R actually does, you can use the `model_matrix()`.

<div class="left", style="width: 50%">

```{r}
sim1
```


</div>

<div class="right", style="width: 50%">

```{r}
model_matrix(sim1, y ~ x)
```

</div>


## Formulae with categorical variables

* It doesn't make sense to parametrize the model with categorical variables,
as we did before. 

* `trans` variable is not a number, so R creates **an indicator column**
that is 1 if "male", and 0 if "female".


<div class="left", style="width: 50%">
```{r}
(df <- tibble(
    sex = c("male", "female", "female", 
            "female", "male", "male"),
     response = c(2, 5, 1, 3, 6, 8)
))
```

</div>

<div class="right", style="width: 50%">

```{r}
model_matrix(df, response ~ sex)
```

</div>



## 

* In general, it creates k−1 columns, where k is the number of categories.

<div class="left", style="width: 50%">

```{r}
(df <- tibble(
    rating = c("good", "bad", "average", "bad",
               "average", "good", "bad", "good"),
    score = c(2, 5, 1, 3, 6, 8, 10, 6)
))
```

</div>

<div class="right", style="width: 50%">

```{r}
model_matrix(df, score ~ rating)
```

</div>

<div class="left", style="width: 100%">

But you don’t need to worry about the parametrization to make predictions.
</div>


## Multiple Linear Regression

Models often include **multiple predictors**, e.g. we might like to predict 
`mpg` using three variables: `wt`, `disp` and `cyl`.

```{r fig.height=5.5, fig.width=12}
ggplot(mtcars, aes(x=wt, y=mpg, col=cyl, size=disp)) + 
    geom_point() +
    scale_color_viridis_c()
```

##

```{r}
mtcars_mult_fit <- lm(mpg ~ wt + disp + cyl, data = mtcars_train)

# Summarize the results 
summary(mtcars_mult_fit)
```


##

To **predict `mpg` for new cars**, you must first create a data frame 
describing the attributes of the new cars, before computing predicted `mpg`
values.

<div class="left", style="width: 50%">

```{r}
newcars <- expand.grid(
    wt = c(2.1, 3.6, 5.1), 
    disp = c(150, 250), 
    cyl = c(4, 6)
)
newcars
```
</div>

<div class="right", style="width: 50%">


```{r}
newcars <- newcars %>% 
    add_predictions(mtcars_mult_fit)
newcars
```

</div>

## Predictions for the test set

```{r}
mtcars_test_mult <- mtcars_test %>% add_predictions(mtcars_mult_fit)
head(mtcars_test_mult, 3)
```

Compute the root mean square error:

$$
RMSE = \frac{1}{\sqrt{n}}\|\vec{y} - \vec{\hat y}\| = 
\sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - \hat y_i)^2}
$$

```{r}
sqrt(mean((mtcars_test_mult$mpg - mtcars_test_mult$pred)^2))
```

## Interaction terms

* An interaction occurs when **an independent variable has a different
effect on the outcome depending on the values of another independent**. 
variable.

* For example, one variable, $x_1$ might have a different effect on $y$ within 
different categories or groups, given by variable $x_2$.

* If you are not familiar with the concept of the interaction terms,
read [this](http://www.medicine.mcgill.ca/epidemiology/joseph/courses/EPIB-621/interaction.pdf).


## Formulas with interactions

In the `sim3` dataset, there is a categorical, `x2`, and a continuous, `x1`,
predictor.

```{r}
ggplot(sim3, aes(x=x1, y=y)) + geom_point(aes(color = x2)) 
```



## Models with interactions

We could fit two different models, one without and one with (`mod2`) 
different slopes and intercepts for each line (for each `x2` category).

<div class="left", style="width: 50%">

```{r}
# Model without interactions:
mod1 <- lm(y ~ x1 + x2, data = sim3)    
# Model with interactions:
mod2 <- lm(y ~ x1 * x2, data = sim3)     
# Generate a data grid for two variables 
# and compute predictions from both models
grid <- sim3 %>% data_grid(x1, x2) %>%   
    gather_predictions(mod1, mod2)
head(grid, 3)
tail(grid, 3)
```

</div>

<div class="right", style="width: 50%">

```{r}
ggplot(sim3, aes(x=x1, y=y, color=x2)) +   
    geom_point() +                       
    geom_line(data=grid, aes(y=pred)) +  
    facet_wrap(~ model)                  
```

</div>

## 

Now, we fit **interaction effects** for the `mtcars` dataset.
Note the '`:`'-notation for the interaction term.
```{r}
mfit_inter <- lm(mpg ~ am * wt, mtcars_train)
names(coefficients(mfit_inter))
summary(mfit_inter)
```


## Exercise 1

</br>

- Go to the "Lec6_Exercises.Rmd" file, which can be downloaded
from the class website under the Lecture tab.

- Complete Exercise 1.


# Lasso Regression

## Choosing a model

* Modern datasets often have "too" many variables, e.g. predict the risk 
of a disease from the single nucleotide polymorphisms (SNPs) data.
* **Issue:** $n \ll p$ i.e. no. of predictors is much larger than than the
no. of observations. 
* **Lasso regression** is especially useful for problems,
where 

> the number of available covariates is extremely large, but
only a handful of them are relevant for the prediction of the outcome.


## Lasso Regression

* Lasso regression is simply regression with $L_1$ penalty. 
* That is, it solves the problem:

\[\boldsymbol{\hat \beta}  = arg \min\limits_{\boldsymbol{\beta}}
\sum_i \left(y^{(i)} 
- \boldsymbol{\beta}^T \boldsymbol{x}^{(i)}\right)^2 + 
\lambda \|\boldsymbol{\beta}\|_1\]

* It turns out that the $L_1$ norm $\|\vec \beta\|_1 = \sum_i |beta_i|$ 
**promotes sparsity**, i.e. only a handful of $\hat\beta_i$ will
actually be non-zero.

* The number of non-zero coefficients depends on the choice of 
the tuning parameter, $\lambda$. The higher the
$\lambda$ the fewer non-zero coefficients.


## `glmnet`

* Lasso regression is implemented in an R package `glmnet`.
* An introductory tutorial to the package can be found 
[here](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html).

```{r, message=FALSE, warning=FALSE}
# install.packages("glmnet")
library(glmnet)
```


##

* We go back to `mtcars` datasets and use Lasso regression
to predict the `mpg` using all variables.
* Lasso will pick a subset of predictors that best predict the `mpg`.
* This means that we technically allow for all variables to be included, but
due to penalization, most of the fitted coefficients will be zero.

```{r}
mtcars <- as.data.frame(mtcars)
class(mtcars)
head(mtcars)
```

## Fitting a sparse model

```{r}
# Convert to 'glmnet' required input format:
y <- mtcars[, 1]  # response vector, 'mpg'
X <- mtcars[, -1] # all other variables treated as predictors
X <- data.matrix(X, "matrix") # converts to NUMERIC matrix

# Choose a training set
set.seed(123)
idx <- sample(1:nrow(mtcars), floor(0.7 * nrow(mtcars)))
X_train <- X[idx, ]; y_train <- y[idx]
X_test <- X[-idx, ]; y_test <- y[-idx]

# Fit a sparse model
fit <- glmnet(X_train, y_train)
names(fit)
```

##

* `glmnet()` compute the Lasso regression for a sequence of 
different tuning parameters, $\lambda$. 
* Each row of `print(fit)` corresponds to a particular
$\lambda$ in the sequence.
* column `Df` denotes the number of non-zero coefficients
(degrees of freedom), 
* `%Dev` is the percentage variance explained, 
* `Lambda` is the value of the currently chosen tuning parameter. 

```{r}
print(fit)
```

##

```{r fig.height=6, fig.width=10}
# label = TRUE makes the plot annotate the curves with the corresponding coeffients labels.
plot(fit, label = TRUE, xvar = "lambda") 
```

* the y-axis corresponds the value of the coefficients.
* the x-axis is denoted "Log Lambda" corresponds to the value of $\lambda$
parameter penalizing the L1 norm of $\boldsymbol{ \hat \beta}$

##

* Each curve corresponds to a single variable, and shows the value
of the coefficient as the tuning parameter varies.
* $\|\hat \beta\|_{L_1}$ increases and $\lambda$
decreases from left to right.
* When $\lambda$ is small (right) there are more non-zero coefficients.


The computed Lasso coefficient for a particular choice of $\lambda$ can be
printed using:

```{r}
# Lambda = 1
coef(fit, s = 1)
```

##

* Like for `lm()`, we can use a function `predict()` to 
predict the `mpg` for the training or the test data. 
* However, we need specify the value of $\lambda$ using
the argument `s`.

```{r}
# Predict for the test set:
predict(fit, newx = X_test, s = c(0.5, 1.5, 2))
```

Each of the columns corresponds to a choice of $\lambda$.

## Choosing $\lambda$

* To choose $\lambda$ can use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).
* Use `cv.glmnet()` function to perform a k-fold cross validation.

> In k-fold cross-validation, the original sample is randomly partitioned into 
k equal sized subsamples. Of the k subsamples, a single subsample is retained 
as the validation data for testing the model, and the remaining k − 1 
subsamples are used as training data. ^[https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation]


##

```{r fig.width=10, fig.height=6}
set.seed(1)
# `nfolds` argument sets the number of folds (k).
cvfit <- cv.glmnet(X_train, y_train, nfolds = 5)
plot(cvfit)
```

* The <span style="color:red">red dots</span> are the average MSE over the k-folds.
* The two chosen $\lambda$ values are the one with $MSE_{min}$ and 
one with $MSE_{min} + sd_{min}$

## 

$\lambda$ with minimum mean squared error, MSE:
```{r}
cvfit$lambda.min
```


The "best" $\lambda$ in a practical sense is usually chosen to be
the biggest $\lambda$ whose MSE is within one standard error of the minimum MSE.
```{r}
cvfit$lambda.1se
```

Predictions using the "best" $\lambda$:

```{r}
final_pred <- predict(cvfit, newx=X_test, s="lambda.1se")
final_pred
```


# More on models 

## Building Models

Building models is an important part of EDA.

It takes practice to gain an intuition for which patterns to look for
and what predictors to select that are likely to have an important effect.

You should go over examples in http://r4ds.had.co.nz/model-building.html
to see concrete examples of how a model is built for `diamonds`
and `nycflights2013` datasets we have seen before.

## Other model families

This chapter has focused exclusively on the class of linear models
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon =
\vec \beta \vec x + \epsilon
\]

and penalized linear models.

There are a large set of other model classes.

## 

Extensions of linear models:
    
* Generalized linear models, `stats::glm()`, binary or count data.
* Generalized additive models, `mgcv::gam()`, extend generalized linear models
to incorporate arbitrary smooth functions.
* Robust linear models, `MASS:rlm()`, less sensitive to outliers.

Completely different models:
    
* Trees, `rpart::rpart()`, fit a piece-wise constant model splitting the 
data into progressively smaller and smaller pieces.
* Random forests, `randomForest::randomForest()`, aggregate many different 
trees.
* Gradient boosting machines, `xgboost::xgboost()`, aggregate trees.


## Useful Books

* ["An introduction to Statistical Learning"](http://www-bcf.usc.edu/~gareth/ISL/getbook.html) [ISL] by James, Witten, Hastie and Tibshirani 

* ["Elements of statistical learning"](http://www.springer.com/gp/book/9780387848570) [ESL] by Hastie, Tibshirani and Friedman 

* ["Introduction to Linear Regression Analysis"](http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470542810.html) by Montgomery, Peck, Vinning




















